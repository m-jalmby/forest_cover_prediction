{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Learning more about NN's by this example of predicting forest cover type. As a baseline we're using a simple random forest, with default parameters. Right now that one is doing better, with an accuracy of ~0.95, whereas the NN is currently sitting at ~0.87\n",
    "\n",
    "## To do\n",
    "\n",
    "Grid over batch sizes and epochs\n",
    "\n",
    "Try difference structures / depths for the NN\n",
    "\n",
    "Study the confusion matrix to see what errors we're making\n",
    "\n",
    "See if there's a connection between the errors and the features, in order to guide possible engineering of new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('cover_data.csv') #Everything looks ok, no missing data, no unexpected data types\n",
    "y = df0['class']\n",
    "y = y - 1 #For the sake of the sparse categorical cross entropy we need labels between 0 and 6\n",
    "df1 = df0.drop(['class'],axis=1) #Drop target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df1.loc[:,df1.nunique()>=50]\n",
    "binary_cols = df1.loc[:,df1.nunique()<50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    plt.figure(figsize = (4,3))\n",
    "    plt.hist(df1[col])\n",
    "    plt.title(col + ', skew = ' + str(skew(df1[col])))\n",
    "    plt.show()\n",
    "    \n",
    "#From the looks of it, these transformations should be useful:\n",
    "#Square: Elevation, Hillshade_9am, Hillshade_Noon\n",
    "#Something for bimodal distributions: Aspect\n",
    "#Square root: Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Horizontal_Distance_To_Fire_Points\n",
    "#No Transformation: Hillshade_3pm\n",
    "\n",
    "left_skewed_transform_cols = ['Elevation', 'Hillshade_9am', 'Hillshade_Noon']\n",
    "right_skewed_transform_cols = ['Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points']\n",
    "bimodal_cols = ['Aspect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy() #Transformed columns\n",
    "\n",
    "for col in left_skewed_transform_cols:\n",
    "    df2[col] = np.square(df2[col])\n",
    "    \n",
    "for col in right_skewed_transform_cols: #sqrt worked better than log and log1p\n",
    "    df2[col] = np.sqrt(df2[col]+(-np.min(df2[col]))+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not perfect, but certainly better\n",
    "for col in num_cols:\n",
    "    #plt.figure(figsize = (4,6))\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "    fig.set_figwidth(10)\n",
    "    fig.set_figheight(4)\n",
    "    axs[0].hist(df1[col])\n",
    "    axs[0].set_xlabel(col + ', ' + str(skew(df1[col])))\n",
    "    plt.title(col)\n",
    "    axs[1].hist(df2[col])\n",
    "    axs[1].set_xlabel(col + ', ' + str(skew(df2[col])))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize data\n",
    "df3 = pd.DataFrame(StandardScaler().fit_transform(df2)) #Normalize\n",
    "df3.columns = df2.columns #Take the name of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df3, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a random forest model, to serve as a baseline\n",
    "\n",
    "#Random Forest\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "#If we want to do a grid search\n",
    "\n",
    "#rf_param_grid = {\n",
    "#    'n_estimators': [50, 100],\n",
    "#    'max_depth': [None, 10, 20],\n",
    "#    'min_samples_split': [2, 10],\n",
    "#    'min_samples_leaf': [1, 4],\n",
    "#    #'max_features': ['auto', 'sqrt']\n",
    "#}\n",
    "\n",
    "#rf_grid_search = GridSearchCV(rf_clf, rf_param_grid,cv=2,scoring='accuracy',verbose=1)\n",
    "#rf_grid_search.fit(X_train,y_train)\n",
    "#best_rf_clf = rf_grid_search.best_estimator_\n",
    "\n",
    "rf_clf.fit(X_train,y_train)\n",
    "#Scoring the best random forest\n",
    "print(rf_clf.score(X_test,y_test))\n",
    "\n",
    "#~0.95 without any parameter optimization, not too bad..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the DNN\n",
    "#Creating the model\n",
    "model = keras.Sequential()\n",
    "#input_layer = layers.InputLayer(input_shape=(X_train.shape[1],))\n",
    "#hidden_layer_1 = layers.Dense(512, activation=\"relu\")\n",
    "hidden_layer_1 = layers.Dense(64, input_dim=X_train.shape[1], activation=\"relu\")\n",
    "hidden_layer_2 = layers.Dense(32, activation=\"relu\")\n",
    "#output_layer = layers.Dense(y.nunique(), activation=\"softmax\")\n",
    "output_layer = layers.Dense(7, activation=\"softmax\")\n",
    "\n",
    "model.add(hidden_layer_1)\n",
    "#model.add(hidden_layer_1)\n",
    "model.add(hidden_layer_2)\n",
    "model.add(output_layer)\n",
    "\n",
    "model.compile(optimizer=\"Adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[SparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test,y_test)\n",
    ")\n",
    "\n",
    "y_true = np.asarray(y_test)\n",
    "probabilities = model.predict(X_test)\n",
    "y_pred = np.argmax(probabilities, axis=1)\n",
    "\n",
    "accuracy_score(y_true, y_pred) #Sitting at 0.866, ways away from the random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "#Display the confusion matrix using seaborn\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
